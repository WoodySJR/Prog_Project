from nltk.corpus import stopwords, wordnet
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk import pos_tag
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import WhitespaceTokenizer
import string
import re
import warnings 
import itertools
class preprocess(object):
    '''
    This class integrates all the text pre-processing functions that we will use. 
    '''
    def __init__(self, stem=False, lemma=False, delstop=False):
        '''
        Initialize the instance. Specify whether to use stemming, lemmatization and 
        stopword deletion. 
        '''
        self.stem = stem # stem or not
        self.lemma = lemma # lemmatize or not
        self.delstop = delstop # delete stopwords or not
        
        if self.delstop:
            self.stop = stopwords.words('english') # load in stopwords if needed
        if self.lemma:
            self.lemmatizer = WordNetLemmatizer() # initialize the lemmatizer if needed
        if self.stem: # initialize the stemmer if needed
            self.stemmer = SnowballStemmer('english')
        
        # raise a warning when lemma and stem are both activated, because they serve similar purposes
        if self.lemma and self.stem: 
            warning_message = \
            'Stemming and lemmatization both activated. You may'+\
            ' want to keep one of them because they serve similar purposes.'+\
            ' Stemming is much simpler and faster, keeping the stem of a word'+\
            '(eg.having->hav), while lemmatization takes into account the'+\
            ' context, and returns the base form of a word according to WordNet'+\
            '(eg. having->have).'
            warnings.warn(warning_message)
        self.tokenizer = WhitespaceTokenizer() # innitialize the tokenizer
            
    def __get_pos(self, pos_tag): 
        '''translate the postags generated by pos_tag to the form required by the lemmatizer
        
            Args: A string of pos tagging
            
            Returns: pos tagging in Wordnet format
        '''
        if pos_tag.startswith('J'):
            return wordnet.ADJ
        elif pos_tag.startswith('V'):
            return wordnet.VERB
        elif pos_tag.startswith('N'):
            return wordnet.NOUN
        elif pos_tag.startswith('R'):
            return wordnet.ADV
        else:
            return wordnet.NOUN
    
    def __remove_repeated_characters(self, s):
        '''
        Replace any letter that appears more than once in a row with only 2 occurrences
        
        Args: a string
        
        Returns: a string with repeated letters reduced to 2 occurrences
        
        '''
        groups = itertools.groupby(s)
        cleaned = ''.join(g[0]*2 if len(list(n))>=2 else g for g,n in groups)
        return cleaned

    def process(self, text):
        '''
        pre-process a piece of text, including: tokenization, removing stopwords, digits and punctuations,
          lemmatization/stemming, and replacing "@username" and weblinks with tokens "USERNAME" and "URL".
          
          Args: a string
          
          Returns: a string after pre-processing(with words joined together by blank)
        '''
        text = text.lower() # lower the text
        words = self.tokenizer.tokenize(text=text) # tokenize sentences into words
        words = ["USERNAME" if word.startswith('@') else word for word in words] # replace usernames
        words = ['URL' if word.startswith('http') else word for word in words] # replace URLs
        words = [word.strip(string.punctuation) for word in words] # remove punctuations
        words = [re.sub(r'[0-9]+', '', word) for word in words] # remove digits
        words = [self.__remove_repeated_characters(word) for word in words] # subsitute consecutive letters with two of them
        words = [word for word in words if len(word)>0] # remove empty tokens
        
        
        if self.delstop: # drop stopwords
            words = [word for word in words if word not in self.stop]
        
        if self.lemma: # lemmatize
            pos_tags = pos_tag(words)
            #print(pos_tags)
            words = [self.lemmatizer.lemmatize(tag[0], self.__get_pos(tag[1])) for tag in pos_tags]
            #print(words)
        
        if self.stem: # stem
            words = [self.stemmer.stem(word) for word in words]
            #print(words)
        
        return(' '.join(words))