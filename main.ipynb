{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c70d8a",
   "metadata": {},
   "source": [
    "# packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e840f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Python class that we wrote for text pre-processing\n",
    "from preprocess import preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b54b9",
   "metadata": {},
   "source": [
    "# PART ONE: data load-in and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('sentiment140/training.csv', encoding='ISO-8859-1',\n",
    "                      header=None) # load-in data\n",
    "training.columns = ['label','id','time','query','username','text'] # remame the columns\n",
    "training.head(5) # display the first 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbf9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = preprocess(lemma=True, stem=False, delstop=True) # innitialize the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg 1\n",
    "print('before:\\n',training['text'][0],'\\n-----------\\n')\n",
    "print('after:\\n',processor.process(training['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg 2\n",
    "print('before:\\n',training['text'][12],'\\n-----------\\n')\n",
    "print('after:\\n',processor.process(training['text'][12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 40,000 instances\n",
    "reduced_data = pd.concat([training[0:20000],training[800000:820000]])\n",
    "reduced_data.index = range(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random splitting\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8,\n",
    "                                 random_state=928)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07481414",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data['status'] = None\n",
    "for train_index, test_index in splitter.split(reduced_data['text'], reduced_data['label']):\n",
    "    reduced_data['status'][train_index] = 't'\n",
    "    reduced_data['status'][test_index] = 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5640ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process: lemma=True, delstop=True\n",
    "reduced_data['lemma&delstop'] = reduced_data.progress_apply(lambda x: processor.process(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112713ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process: lemma=True, delstop=False\n",
    "processor2 = preprocess(lemma=True, stem=False, delstop=False) \n",
    "reduced_data['lemma'] = reduced_data.progress_apply(lambda x: processor2.process(x['text']), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process: lemma=False, delstop=True\n",
    "processor3 = preprocess(lemma=False, stem=False, delstop=True) \n",
    "reduced_data['delstop'] = reduced_data.progress_apply(lambda x: processor3.process(x['text']), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process: lemma=False, delstop=False\n",
    "processor4 = preprocess(lemma=False, stem=False, delstop=False) \n",
    "reduced_data['none'] = reduced_data.progress_apply(lambda x: processor4.process(x['text']), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.to_csv('40k_split_processed.csv',index=False) # save the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce124b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "655391d7",
   "metadata": {},
   "source": [
    "# PART TWO: Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808151af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('40k_split_processed.csv') # load in the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2395a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function for drawing wordclouds,\n",
    "#  which is customized for our data\n",
    "from custom_wordcloud import custom_wordcloud as cwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a85092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further drop some meaningless words to improve visualization\n",
    "drops = ['quot','today','day','now','ing',' go ',' got ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing wordclouds for positive and negative tweets separately\n",
    "fig=plt.figure(figsize=(10,10),dpi=300)\n",
    "ax1=fig.add_subplot(121)\n",
    "ax1.imshow(cwc(data=data, label='positive', drop=drops, color='autumn'))\n",
    "plt.axis(\"off\")\n",
    "ax2=fig.add_subplot(122)\n",
    "ax2.imshow(cwc(data=data, label='negative', drop=drops, color='winter'))\n",
    "plt.axis(\"off\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e30cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de5e396e",
   "metadata": {},
   "source": [
    "# PART THREE: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Class where we implement Naive Bayes from scratch\n",
    "from naive_bayes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be916d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and validation data\n",
    "train_set = data[data['status']=='t']\n",
    "train_set.index = range(len(train_set))\n",
    "test_set = data[data['status']=='v']\n",
    "test_set.index = range(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a grid to search\n",
    "techniques = ['lemma&delstop','lemma','delstop','none']\n",
    "vocab_sizes = [100,500,1000,2000,5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce617b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to store results\n",
    "NB_results = pd.DataFrame(np.zeros((5,4)))\n",
    "NB_results.index = vocab_sizes\n",
    "NB_results.columns = techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a207870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize processors\n",
    "processors = {}\n",
    "for t in techniques:\n",
    "    lemma, delstop = False, False\n",
    "    if 'lemma' in t:\n",
    "        lemma=True\n",
    "    if 'delstop' in t:\n",
    "        delstop=True\n",
    "    processors[t] = preprocess(lemma=lemma, stem=False, delstop=delstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "for i,t in enumerate(techniques):\n",
    "    model = naive_bayes(5, None) # words that appear no more than five times are discarded\n",
    "    for j,v in enumerate(vocab_sizes):\n",
    "        print('='*10+t+' '+str(v)+'='*10)\n",
    "        model.vocab_size = v\n",
    "        model.train(train_set[t][train_set['label']==4], train_set[t][train_set['label']==0]) # training\n",
    "        # path='NB/'+str(t)+str(v)+'.png'\n",
    "        result = model.evaluate(test_set, processors[t], prob=False, roc=False) # evaluation\n",
    "        NB_results.iloc[j,i] = result['AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_results.to_csv('NB_AUC.csv') # save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cc70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a98dd6f6",
   "metadata": {},
   "source": [
    "# PART FOUR: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27338077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib import text\n",
    "from mxnet import nd, autograd\n",
    "import collections\n",
    "from mxnet.gluon import data as gdata,loss as gloss, utils as gutils, nn, rnn\n",
    "import d2lzh as d2l\n",
    "from mxnet import init, gluon\n",
    "import mxnet as mx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb517749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions that we need to train a neural network\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8936990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network architecture\n",
    "class lstm(nn.Block):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers_lstm, num_layers_ffn, **kwargs):\n",
    "        super(lstm, self).__init__(**kwargs)\n",
    "        \n",
    "        # number of hidden units\n",
    "        self.num_hiddens = num_hiddens\n",
    "        \n",
    "        # lstm layer\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size) # create the projection layer\n",
    "        self.encoder = rnn.LSTM(hidden_size=num_hiddens, num_layers=num_layers_lstm, bidirectional=True,\n",
    "                               input_size=embed_size)\n",
    "        \n",
    "        # output layers\n",
    "        self.ffn = nn.Sequential()\n",
    "        for k in range(num_layers_ffn):\n",
    "            self.ffn.add(nn.Dense(units=128, activation='relu'))\n",
    "        self.output = nn.Dense(units=2)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs.T) # project token indices to word embeddings\n",
    "        outputs = self.encoder(embeddings).transpose((1,0,2)) # (#words, batch_size, 2*num_hiddens)->(batch_size,#words,2*num_hiddens)\n",
    "        outputs = nd.concat(outputs[:,0,self.num_hiddens:2*self.num_hiddens].squeeze(), \n",
    "                  outputs[:,outputs.shape[1]-1,0:self.num_hiddens].squeeze(), dim=1) # keep the encoding of the first and last tokens (batch_size, 2*num_hiddens)\n",
    "        \n",
    "        outputs = self.ffn(outputs) # (batch_size, 128)\n",
    "        outputs = self.output(outputs) # (batch_size, 2)\n",
    "        return(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['technique'] = None\n",
    "results['num_hiddens'] = None\n",
    "results['num_layers_lstm'] = None\n",
    "results['num_layers_ffn'] = None\n",
    "results['epoch'] = None\n",
    "results['AUC'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "techniques = ['lemma&delstop','lemma','delstop','none']\n",
    "nums_hiddens = [64,128,256]\n",
    "nums_layers_lstm = [1,2]\n",
    "nums_layers_fnn = [1,2]\n",
    "ctx = d2l.try_gpu()\n",
    "for t in techniques:\n",
    "    iter_train, iter_test, emb, vocab = get_features(t)\n",
    "    for h in nums_hiddens:\n",
    "        for l1 in nums_layers_lstm:\n",
    "            for l2 in nums_layers_fnn:\n",
    "                print(t,' ', h, ' ', l1, ' ', l2)\n",
    "                ctx.empty_cache()\n",
    "                net1 = lstm(vocab=vocab, embed_size=300, num_hiddens=h, num_layers_lstm=l1, num_layers_ffn=l2)\n",
    "                mx.random.seed(2022) # set the random seed to ensure replicability\n",
    "                net1.initialize(init.Xavier(), ctx=ctx)\n",
    "                net1.embedding.weight.set_data(emb.idx_to_vec)\n",
    "                trainer = gluon.Trainer(net1.collect_params(), 'adam', {'learning_rate': 0.001, 'wd':0.00001})\n",
    "                train(iter_train, iter_test, net1, trainer, ctx, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('lstm_AUC.csv', index=False) # save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe90092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76edb1c6",
   "metadata": {},
   "source": [
    "# PART FIVE: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946007ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocabulary of BERT\n",
    "_,vocab = nlp.model.get_model(name='bert_12_768_12', dataset_name='book_corpus_wiki_en_uncased',\n",
    "                    pretrained=True,use_pooler=False,use_classifier=False,\n",
    "                    use_token_type_embed=False, use_decoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute our special tokens \"USERNAME\" and \"URL\" into the vocab\n",
    "index1 = vocab.token_to_idx[\"[unused10]\"]\n",
    "vocab.token_to_idx['USERNAME'] = index1\n",
    "index2 = vocab.token_to_idx[\"[unused11]\"]\n",
    "vocab.token_to_idx['URL'] = index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model architecture\n",
    "class BERT(nn.Block):\n",
    "    def __init__(self, num_layers_ffn, **kwargs):\n",
    "        super(BERT, self).__init__(**kwargs)\n",
    "        \n",
    "        # bert encoder\n",
    "        self.encoder,_ = nlp.model.get_model(name='bert_12_768_12',\n",
    "                                           dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                           pretrained=True,\n",
    "                                           ctx=mx.gpu(),\n",
    "                                           use_pooler=False,\n",
    "                                           use_classifier=False,\n",
    "                                           use_token_type_embed=False,\n",
    "                                           use_decoder=False,\n",
    "                                           dropout=0)\n",
    "        # output layers\n",
    "        self.ffn = nn.Sequential()\n",
    "        for k in range(num_layers_ffn):\n",
    "            self.ffn.add(nn.Dense(units=128, activation='relu'))\n",
    "        self.output = nn.Dense(units=2)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        N = inputs.shape[0]\n",
    "        token_types = nd.zeros((N, max_len), ctx=mx.gpu())\n",
    "        outputs = self.encoder(inputs, token_types)\n",
    "        outputs = outputs.mean(axis=1) # average pooling\n",
    "        outputs = self.ffn(outputs) # (batch_size, 128)\n",
    "        outputs = self.output(outputs) # (batch_size, 2)\n",
    "        return(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['technique'] = None\n",
    "results['num_layers_ffn'] = None\n",
    "results['epoch'] = None\n",
    "results['AUC'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "techniques = ['none','lemma&delstop','lemma','delstop']\n",
    "nums_layers_fnn = [1,2]\n",
    "ctx = d2l.try_gpu()\n",
    "for t in techniques:\n",
    "    iter_train, iter_test = get_features_2(t)\n",
    "    for l2 in nums_layers_fnn:\n",
    "        print(t,' ',l2)\n",
    "        ctx.empty_cache()\n",
    "        net2 = BERT(num_layers_ffn=l2)\n",
    "        mx.random.seed(2022) # set the random seed to ensure replicability\n",
    "        net2.initialize(init.Xavier(), ctx=ctx)\n",
    "        trainer = gluon.Trainer(net2.collect_params(), 'adam', {'learning_rate': 0.0001})\n",
    "        train(iter_train, iter_test, net2, trainer, ctx, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fb7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('BERT_AUC.csv', index=False) # save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65199e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c0d2dd4",
   "metadata": {},
   "source": [
    "# PART SIX: Visualization of grid search results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798be8f5",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_NB = pd.read_csv('NB_AUC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "ax.spines['right'].set_visible(False) \n",
    "ax.spines['top'].set_visible(False) \n",
    "colors = ['orange','green','purple','blue','red']\n",
    "for i in range(0,5):\n",
    "    plt.plot(AUC_NB.iloc[i,1:5], label=AUC_NB.iloc[i,0],color=colors[i])\n",
    "plt.legend(title='size of vocabulary')\n",
    "plt.xticks([0,1,2,3],['lemmatization and \\nstopword deletion',\n",
    "           'lemmatization',\n",
    "           'stopword deletion',\n",
    "           'neither'],fontsize=10)\n",
    "plt.xlabel('text pre-processing technique',fontsize=15)\n",
    "plt.ylabel('AUC',fontsize=15)\n",
    "plt.savefig('nb.png',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e56a7",
   "metadata": {},
   "source": [
    "## 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_lstm = pd.read_csv('lstm_AUC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b71dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 lstm layer, 1 output layer\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "ax.spines['right'].set_visible(False) \n",
    "ax.spines['top'].set_visible(False) \n",
    "for i in [64,128,256]:\n",
    "    index1 = AUC_lstm['num_layers_lstm']==1\n",
    "    index2 = AUC_lstm['num_layers_ffn']==1\n",
    "    index3 = AUC_lstm['num_hiddens']==i\n",
    "    sub = AUC_lstm[index1*index2*index3]\n",
    "    sub = sub.groupby('technique').max()\n",
    "    sub = sub.loc[['lemma&delstop','lemma','delstop','none'],:]\n",
    "    plt.plot(sub.AUC,label=i)\n",
    "plt.legend(title='number of hidden units',loc='lower right')\n",
    "plt.xticks([0,1,2,3],['lemmatization and \\nstopword deletion',\n",
    "           'lemmatization',\n",
    "           'stopword deletion',\n",
    "           'neither'],fontsize=10)\n",
    "plt.xlabel('text pre-processing technique',fontsize=15)\n",
    "plt.ylabel('AUC',fontsize=15)\n",
    "plt.savefig('lstm1.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 lstm layer, 2 output layer\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.spines['right'].set_visible(False) \n",
    "ax.spines['top'].set_visible(False)\n",
    "for i in [64,128,256]:\n",
    "    index1 = AUC_lstm['num_layers_lstm']==1\n",
    "    index2 = AUC_lstm['num_layers_ffn']==2\n",
    "    index3 = AUC_lstm['num_hiddens']==i\n",
    "    sub = AUC_lstm[index1*index2*index3]\n",
    "    sub = sub.groupby('technique').max()\n",
    "    sub = sub.loc[['lemma&delstop','lemma','delstop','none'],:]\n",
    "    plt.plot(sub.AUC,label=i)\n",
    "plt.legend(title='number of hidden units',loc='lower right')\n",
    "plt.xticks([0,1,2,3],['lemmatization and \\nstopword deletion',\n",
    "           'lemmatization',\n",
    "           'stopword deletion',\n",
    "           'neither'],fontsize=10)\n",
    "plt.xlabel('text pre-processing technique',fontsize=15)\n",
    "plt.ylabel('AUC',fontsize=15)\n",
    "plt.savefig('lstm2.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f49a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 lstm layer, 1 output layer\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "ax.spines['right'].set_visible(False) \n",
    "ax.spines['top'].set_visible(False)\n",
    "for i in [64,128,256]:\n",
    "    index1 = AUC_lstm['num_layers_lstm']==2\n",
    "    index2 = AUC_lstm['num_layers_ffn']==1\n",
    "    index3 = AUC_lstm['num_hiddens']==i\n",
    "    sub = AUC_lstm[index1*index2*index3]\n",
    "    sub = sub.groupby('technique').max()\n",
    "    sub = sub.loc[['lemma&delstop','lemma','delstop','none'],:]\n",
    "    plt.plot(sub.AUC,label=i)\n",
    "plt.legend(title='number of hidden units',loc='lower right')\n",
    "plt.xticks([0,1,2,3],['lemmatization and \\nstopword deletion',\n",
    "           'lemmatization',\n",
    "           'stopword deletion',\n",
    "           'neither'],fontsize=10)\n",
    "plt.xlabel('text pre-processing technique',fontsize=15)\n",
    "plt.ylabel('AUC',fontsize=15)\n",
    "plt.savefig('lstm3.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66738e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 lstm layer, 2 output layer\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "ax.spines['right'].set_visible(False) \n",
    "ax.spines['top'].set_visible(False) \n",
    "for i in [64,128,256]:\n",
    "    index1 = AUC_lstm['num_layers_lstm']==2\n",
    "    index2 = AUC_lstm['num_layers_ffn']==2\n",
    "    index3 = AUC_lstm['num_hiddens']==i\n",
    "    sub = AUC_lstm[index1*index2*index3]\n",
    "    sub = sub.groupby('technique').max()\n",
    "    sub = sub.loc[['lemma&delstop','lemma','delstop','none'],:]\n",
    "    plt.plot(sub.AUC,label=i)\n",
    "plt.legend(title='number of hidden units',loc='lower right')\n",
    "plt.xticks([0,1,2,3],['lemmatization and \\nstopword deletion',\n",
    "           'lemmatization',\n",
    "           'stopword deletion',\n",
    "           'neither'],fontsize=10)\n",
    "plt.xlabel('text pre-processing technique',fontsize=15)\n",
    "plt.ylabel('AUC',fontsize=15)\n",
    "plt.savefig('lstm4.png',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bd8f6",
   "metadata": {},
   "source": [
    "## 3. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27507f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_BERT = pd.read_csv('BERT_AUC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "ax.spines['right'].set_visible(False) \n",
    "ax.spines['top'].set_visible(False) \n",
    "for i in [1,2]:\n",
    "    sub = AUC_BERT[AUC_BERT['num_layers_ffn']==i]\n",
    "    sub = sub.groupby('technique').max()\n",
    "    sub = sub.loc[['lemma&delstop','lemma','delstop','none'],:]\n",
    "    plt.plot(sub.AUC,label=i)\n",
    "plt.legend(title='number of output layers',loc='lower right')\n",
    "plt.xticks([0,1,2,3],['lemmatization and \\nstopword deletion',\n",
    "           'lemmatization',\n",
    "           'stopword deletion',\n",
    "           'neither'],fontsize=10)\n",
    "plt.xlabel('text pre-processing technique',fontsize=15)\n",
    "plt.ylabel('AUC',fontsize=15)\n",
    "plt.savefig('bert.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da4930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af465cba",
   "metadata": {},
   "source": [
    "# PART SEVEN: final training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee28ad9",
   "metadata": {},
   "source": [
    "## 1. data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29120e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_csv('test.csv',header=None)\n",
    "final_test.columns = ['label','query','time','ip','username','text']\n",
    "final_test = final_test[final_test['label']!=2] # only keep positive and negative examples\n",
    "final_test.index = range(len(final_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cec397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "words = [str(st).split(' ') for st in data['none']]\n",
    "words_idx = [vocab[x] for x in words]\n",
    "max_len = max([len(words[i]) for i in range(len(words))])\n",
    "features_train = nd.array([pad(x) for x in words_idx])\n",
    "labels_train = nd.array([1 if data['label'][i]==4 else 0 for i in range(len(data))])\n",
    "\n",
    "dataset_train = gdata.ArrayDataset(features_train,labels_train)\n",
    "iter_train = gdata.DataLoader(dataset_train,256,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "processor = preprocess(lemma=False, stem=False, delstop=False)\n",
    "final_test['none'] = [processor.process(final_test['text'][i]) for i in range(len(final_test))]\n",
    "\n",
    "words = [str(st).split(' ') for st in final_test['none']]\n",
    "words_idx = [vocab[x] for x in words]\n",
    "features_test = nd.array([pad(x) for x in words_idx])\n",
    "\n",
    "labels_test = nd.array([1 if final_test['label'][i]==4 else 0 for i in range(len(final_test))])\n",
    "\n",
    "dataset_test = gdata.ArrayDataset(features_test,labels_test)\n",
    "iter_test = gdata.DataLoader(dataset_test,256,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c4e9e",
   "metadata": {},
   "source": [
    "## 2. model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = BERT(num_layers_ffn=1)\n",
    "mx.random.seed(2022) # set the random seed to ensure replicability\n",
    "net3.initialize(init.Xavier(), ctx=ctx)\n",
    "trainer = gluon.Trainer(net3.collect_params(), 'adam', {'learning_rate': 0.0001})\n",
    "train(iter_train, iter_test, net3, trainer, ctx, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
